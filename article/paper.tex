\section{Introduction}
The best paper in the world is~\cite{Liben-Nowell:2003:LPP:956863.956972}.

\input link-prediction

\section{Implementation}
The selected algorithms were implemented and tested over three different 
database systems: Relational, Key-Value store and Graph. 
In each system we attempted to implement the algorithms as natively as possible, 
using that system's features and applying common optimizations. 

For the bechmarking we chose MySQL, Redis and Neo4J.
These databases are all open-source, enabling us to publish these benchmarks,
and popular in their domain, therefore considered as a viable option for 
such social network implementation.

\subsection{MySQL}
MySQL is a popular multi-platform, open-source relational database management
system, sponsored by Oracle. It can be used in client-server or embedded
architecture and it's main features include: 
\begin{itemize}
	\item {\bf Pluggable storage-engines:} MySQL supports several storage engines, 
		most notables are MyISM (fast, simple, no ACID support, no foreign keys) and
		InnoDB (default, supports transactions, ACID and foreign keys).
	\item {\bf Stored Procedures} Like most other relational databases MySQL supports 
		stored procedures execution. Advantages of this feature include 
		avoidance of network traffic for the SQL queries and intermediate results,
		and encapsulation of a multi-query logic in one place.
	\item {\bf Fast Bulk Data Loading} MySQL support very fast data loading from file,
		with it's 'LOAD DATA' statement. From all the systems we tested, this offered 
		the easiest and fastest way to load our graph.
\end{itemize}

In this case, the algorithms were implemented as a SQL stored-procedures,
using indices and intermediate tables when possible. A Python script was used
to run these stored-procedures and collect the benchmarks. For a storage-engine 
we chose InnoDB, the default engine in MySQL 5.5, as it out-performs MyISM in 
most senarios. \footnote{Sara: can we cite Oracle white papers? 
http://www.oracle.com/partners/en/knowledge-zone/mysql-5-5-innodb-myisam-522945.pdf}
\linebreak

Implementation details:
\begin{itemize}
	\item {\bf Helper Tables} 
        We generated the following helper tables to speed up some algorithms:
		\begin{itemize}
			\item {common-neighbors (id1, id2, neighbor)}
			\item {common-neighbors-count (id1, id2, number of common neighbors)}
			\item {neighbors (id, number of neighbors)}
			\item {topN (id, number of neighbors)}
				This table is simply the top 101 items of the neighbors table.
		\end{itemize}
	\item {\bf Common Neighbors (global)} 
		To find the top N records with most common neighbors, 
		we selected them from the common-neighbors-count table.
	\item {\bf Jaccard's Coefficient (global)} 
		We used the following relation between the cardinality of union and intersection:
		\[{|N(x) \cup N(y)|} = {|N(x)| + |N(y)| - |N(x) \cap N(y)|}\]
		So to calculate the score we used the common-neighbors-count and 
		the neighbors helper tables.
	\item {\bf Adamic/Adar (global)} 
		We used the common-neighbors joined with the neighbors helper table.
	\item {\bf Preferential attachment (Global)}
		We used only the topN helper table, because the highest products 
		must be combinations of the nodes with most neighbors.
	\item {\bf Common Neighbors (for node)}
		We used a simple join on the edges table, no helper tables required.
	\item {\bf Jaccard's Coefficient (for node)}
		We used the relation between the cardinality of union and intersection,
		with join of two edges tables, and two neighbors helper tables.
	\item {\bf Adamic/Adar (for node)}
		We used join on the edges table, with the neighbors helper table.
	\item {\bf Preferential attachment (for node)}
		We used the topN helper table, just like in the global version.
	\item {\bf Graph Distance (for node)}
		We used a recursive procedure, starting with a temporary table with all 
		the node's neighbors, and on each iteration N we added all the neighbors 
		of nodes in depth N-1. This implementation required us to reconfigure 
		MySQL maximum recursion depth.
	\item {\bf Katz (for node)}
		We used a recursive procedure to calculate $|P^l(x,y)|$ for all reachable 
		nodes in depth 3, and stored that in a temporary table. We then used that 
		to calculate the score.
	\item {\bf Rooted PageRank (for node)}
        We used an iterative loop over a temporary table <id, rpr, new-rpr>, 
		initiated with all the nodes in the graph, and 1/N value where N is the 
		number of nodes in the graph. On every iteration we updated the 
		new-rpr value, using the formula above. We stopped when the algorithm
        converged on the top-10 scoring nodes and their order.
\end{itemize}

\subsection{Redis}
Redis is an open-source, in-memory key-value store database system, sponsored
by VMware. It's written in C and has atomic operations, we used the following features:
\begin{itemize}
	\item {\bf Transactions}
        You can use redis transaction to save time spent on network trafic 
        between the client API and the server. We used that feature to speed up
        the graph load significantly.
	\item {\bf Data structure as value}
        Redis support storing many types of data structures as valuse and 
        performe efficient, atomic operation over them. I.e. there are strings,
        hashes (key-value pairs), lists, sets (in which we used to store the 
        adjacency lists), and sorted-sets (like sets, but with rank for each item).
        Operations include: incrementing value in hash, appending to lists, 
        union/intersection/difference on sets, and getting sorted-set member with top rank.
	\item {\bf Lua Scripting}
        Lua scripting enable you to run your own code, in an atomic matter inside the server.
        Unlike transactions, you can add non-Redis logic to this atomic operation. We used 
        Lua scripting to speed-up and algorithm which took long time to run. \footnote{Sara: 
        we are kind of abusing this feature, by locking the db and assuming single client, 
        should we mention that?}
\end{itemize}

Here, some of the algorithms were implemented in Python using redis API, and
some were implemented as Lua scripts (to minimize Redis API calls). When an
index was needed, we simply used another Redis DB (you get 16 with the default
configuration). 
The graphs were represented using adjacency lists, i.e. a key is a node-id and
it's value is a set of his neighbors. We also kept a nodes database <id, name>, 
basicall just to get a node count (i.e. needed for the Rooted PageRank implementation),
because redis can't hold empty sets.
\linebreak

Implementation details:
\begin{itemize}
	\item {\bf Helper Database} 
        We only generated one database speed up the Preferential attachment algorithm:
	    topN (id, number of neighbors), a database which holds 101 nodes with most neighbors.
	\item {\bf Common Neighbors (global)} 
        We implemented this algorithm using the Lua scripting feature. First we accumulate 
        the number of common neighbors for each node pair, then iterated over the results 
        to keep the top scoring, and return a sorted result.
	\item {\bf Jaccard's Coefficient (global)}
        Using Lua scripting we used the first part of the Common Neighbors implementation,
        and the relation between the cardinality of union and intersection, and that redis
        can give us the cardinality of a set in a single operation (i.e. number of neighbors)
        we calculated the score and continued similarly to the above implementation.
	\item {\bf Adamic/Adar (global)} 
        Using the Lua scripting, we accumulated list of common neighbors for each pair,
        then went over that list and calculated the algorithm, keeping only the top scoring pairs.
	\item {\bf Preferential attachment (Global)}
        We used the helper database that we created beforehand, to find pairs 
		with maximal product.
	\item {\bf Common Neighbors (for node)}
        We used the same logic as with the global implementation, but with the 
		Python API, as it was fast enough.
	\item {\bf Jaccard's Coefficient (for node)}
        We used the same logic as with the global implementation, but with the 
		Python API, as it was fast enough.
	\item {\bf Adamic/Adar (for node)}
        We used the same logic as with the global implementation, but with the 
		Python API, as it was fast enough.
	\item {\bf Preferential attachment (for node)}
        We used the same logic as with the global implementation.
	\item {\bf Graph Distance (for node)}
        Using the Python API and an iterative loop, we started with the source 
		node as depth 0, on each iteration we expended all the nodes from the 
		previous depth.
	\item {\bf Katz (for node)}
        Using Lua, we started by calculating the $|P^l(x,y)|$ for all reachable 
		nodes in depth 3, and then used that to calculate the score.
	\item {\bf Rooted PageRank (for node)}
        Using Lua, we used an iterative loop over a temporary 
		table <id, rpr, new-rpr>, initiated with all the nodes in the graph, 
		and 1/N value where N is the number of nodes in the graph. On every 
        iteration we updated the new-rpr value, using the formula above. 
		We stopped when the algorithm converged on the top-10 scoring nodes 
		and their order.
\end{itemize}

\subsection{Neo4J}
Neo4J is a multi-platform, open-source, graph database supported by Neo
Technology. Some of it's main features are:
\begin{itemize}
    \item {\bf Transactions}
        Neo4J supports ACID transactions, similar to RDBMS. This also enables 
        you to send chunk of commands to the database instead of single 
        instactions and avoid network latency for each call. We tried to use
        this to load our graph, but this method is just too slow for bulk insertions.
    \item {\bf Embedded mode}
        Embedding Neo4J in your application mainly saves time on REST calls 
        and enables fast interaction with the database. We used that feature 
        to load the graph, and then move the generated files to the server.
        We found that currently, this is the fastest way to bulk insert a 
        large graph into a Neo4J server.
    \item {\bf Cypher query language}
        Cypher is a declerative query language for graphs, enabling you to
        match pattern, filter on properties and make changes. It's syntax is
        much like SQL, e.g. friends of friends which are not directly connected:
        \begin{verbatim}
            START n=node(...)
            MATCH n-->m-->o
            WHERE not ( n-->o )
            RETURN o
        \end{verbatim}
    \item {\bf Indexing}
		There are two main indexing options in Neo4J:
		\begin{enumerate}
			\item {\bf Using the graph}
				An index can be implemented as a layed over the graph, 
				i.e. by adding more relations which represent the index
				structure.
			\item {\bf Separate index}
				By default, Neo4J uses Apache Lucene as a backend index engine.
				This enables you to index Nodes and Relations, and use all Lucene's
				features, including fulltext index.
				We used this form of indexing to speed-up the Preferential attachment
				algorithm.
		\end{enumerate}
\end{itemize}

We attempted to implement all the algorithms using Cypher. Though this was 
usually an easy task, some of the language current limitations prevented us 
from achiving that goal:
\begin{itemize}
	\item Lack of major math functions (e.g. Power, Log, Rand), and no support 
		for user defined function, prevented from implementing the Adamic/Adar 
		algorithm or to use multiple queries, joined by logic in Python, 
		to implement the Katz algorithm.
	\item Performance issues on mutiple start nodes, in particular when matching
		a non-trivial pattern over the whole graph (i.e. our global benchmarks).
		We had to omit the global version of Jaccard Coefficient, and find another
		approach to the global Common Neighbors implementation.
\end{itemize}

Implementation details:
\begin{itemize}
	\item {\bf Helper Neighbor count property}
		Since Neo4J doen't provide a fast way to get the number of node's neighbors,
		we added a neighbor count property to each node. This was useful for 
		almost any algorithm we implemented.
	\item {\bf Helper Index}
		We generated an Lucene index containing 101 nodes with most neighbors,
		to speed up the Preferential attachment algorithm.
	\item {\bf Common Neighbors (global)} 
		In order to circumvent a Cypher performance issue with multiple start 
		nodes, we used a different approach to the Common Neighbors implementation:
		we realized that the result-set of such query will only contain nodes with
		at least $\alpha$ number of neighbors, where $\alpha$ is number of neighbors
		of the lowers scoring pair in the result-set.
		So we used a binary search method to find that $\alpha$, by iteratively 
		running the	query over nodes with $\beta$ number of neighbors, where $\beta$
		is initiated to $M/2$ ($M$ is the maximum number of neighbors a node in the
		graph has), divding $\beta$ by two each iteration, until $\alpha > \beta$.
	\item {\bf Preferential attachment (Global)}
        We used the helper index that we created beforehand, to find pairs with 
		maximal product.
	\item {\bf Common Neighbors (for node)}
        We used a simple query.
	\item {\bf Jaccard's Coefficient (for node)}
        We used a simple query, using the relation between the cardinality of 
		union and intersection
	\item {\bf Preferential attachment (for node)}
        We used the helper index that we created beforehand, to find pairs with 
		maximal product.
	\item {\bf Graph Distance (for node)}
		We iteratively ran a query, returning nodes until depth $i$ (increased on 
		every iteration), until depth limit or returend data set limit has met.
	\item {\bf Katz (for node)}
		Without power function support in Cypher, we had to use multiple queries,
		to calculate the Katz algorithm.
	\item {\bf Rooted PageRank (for node)}
        We used an iterative loop with the same logic like the other two databases,
		storing the intermediate values as node properties.
\end{itemize}

\section{Experiments}
We benchmarked the above implementations using 10 undirected graphs:
\begin{itemize}
	\item 
		Three graphs were extracted from the DBLP XML records using the authors 
		as nodes, and co-authorship relation as edges. Each graph was over a 
		different time period, and  we only kept authors with 3 or more 
		publication within the selected time period (core-3). 
	\item 
		Seven graphs from the Stanford Network Analysis Project (SNAP) dataset
		collection:
		\begin{enumerate}
			\item {ego-Facebook} 
				Anonymized Facebook data that was collected from survey 
				participants using a Facebook app.
			\item {email-Enron} 
				Enron email communication network, released by the 
				Federal Energy Regulatory Commission during its investigation.
			\item {ca-AstroPh}
				Scientific collaborations between authors papers submitted to 
				Astro Physics category.
			\item {ca-CondMat}
				Scientific collaborations between authors papers submitted to 
				Condense Matter category.
			\item {ca-GrQc}
				Scientific collaborations between authors papers submitted to 
				General Relativity and Quantum Cosmology category.
			\item {ca-HepPh}
				Scientific collaborations between authors papers submitted to 
				High Energy Physics - Phenomenology category.
			\item {ca-HepTh}
				Scientific collaborations between authors papers submitted to 
				High Energy Physics - Theory category
		\end{enumerate}
\end{itemize}

\begin{tabular}{ |l||c|r| }
	\hline
	Data-set & Node Count & Edge Count \\
	\hline
	dblp-2002-2009-core3 & 182493 & 1621846 \\
	dblp-2010-2012-core3 & 248695 & 2589320 \\
	dblp-all-core3       & 366600 & 4349796 \\
	ego-Facebook         & 4039   & 170174  \\
	email-Enron          & 36692  & 367662  \\
	ca-AstroPh           & 18771  & 396160  \\
	ca-CondMat           & 23133  & 186936  \\
	ca-GrQc              & 5241   & 28980   \\
	ca-HepPh             & 12006  & 237010  \\
	ca-HepTh             & 9875   & 51971   \\
	\hline  
\end{tabular}

Graphs to add:
\begin{enumerate}
\item Top N index generation (significantly faster on MySQL)
\item Graph Distance (significantly faster on Redis)
\item Jaccard Coefficient (faster on Neo4J)
\end{enumerate}

\section{Conclusions}
The main conclusion we took from this experience is that there is no silver-bullet for storing and running algorithms over social-network graphs. Generally speaking, MySQL seems to have an edge with indices-based implementation, Redis seems to be extremely flexible and open to endless optimization, and Neo4J's Cypher is leading with implementation simplicity.
If we were to build a social network database we'd consider holding replicas in server types of system. While this solution might not be disk-space efficient, it'll provide us the opportunity to achieve the fastest query time with minimal time investment.

Please note that we didn't use any scale out features. A large network implementation can't pick a solution which doesn't have it. \#TODO
